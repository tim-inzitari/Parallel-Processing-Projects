Throughout this semester we have been running the same parallelized algorithm in several different Parrallel API's: MPI, PThreads and OPEN MP. For each program I used the same AMD Ryzen 9 3900x 12-Core Processor at a standard clock of 3.80 GHz.

The largest difference between these three api's were the scalability of the systems as we increased both processor/thread count and array size. The MPI variant we found to have very low scalability with incredibly weak performance at the 8 processor count. We found PThreads to be weakly scalable with fall off as we increased the array size. The OpenMP system was found to be strongly scalable with the efficiency remaining near 1 at all levels tested


Another difference was the runtimes themselves, while less efficient the runtime of MPI was around 10^2 times faster than PThreads and OpenMP at every level we tested. With it clocking it with times around X * 10^-5 times, and OPENMP and PThreads going at times of X * 10^-3.  In comparing OpenMP and Pthreads we find that on average Pthreads are faster than the OpenMP Counterparts.

As for speedup, we find similar results to the efficiency with OpenMP Coming on top, with Pthreads following closely behind until you reach the 16000 array sizes. and then MPI falling far behind the pack. 


With these in mind the main conclusion I had, is that dynamic memory systems like MPI will run faster than their shared memory partners, however they are far less efficient. So the correct use for each scenario will depend on how important efficiency is compared to speed. This allows all the APIs to have their own specific usecases depending on what you need.